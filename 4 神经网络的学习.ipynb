{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 从数据中学习\n",
    "神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指神经网络可以通过数据自行决定最合适的权重参数。\n",
    "## 4.1.1数据驱动\n",
    "数据是机器学习的命根子。从数据中寻找答案、从数据中发现模式、根据数据讲故事...\n",
    "\n",
    "数据驱动的方法，脱离了过往以人为中心的方法。\n",
    "\n",
    "传统的机器学习，需要先由认为提取数据的特征量，然后再由SVM,KNN等分类器进行学习。然而在神经网络中，不需要认为提取特征量，神经网络直接进行从数据到输出的学习，提取特征量的过程都是由神经网络来实现的。（也因此，深度学习有时也被称为端到端的机器学习，端到端的意思也就是从原始数据（输入）到目标结果（输出）的意思）\n",
    "## 4.1.2训练数据和测试数据\n",
    "在机器学习过程中，一般将数据分类训练数据（或者称为监督数据）和测试数据两部分来进行学习和推理。为了正确评估模型的泛化能力和避免模型过拟合，必须划分训练数据和测试数据。\n",
    "\n",
    "补充：模型只对某个数据集过度拟合的状态称为*过拟合*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 损失函数\n",
    "就像我们可以使用一个数值指标来判度自己的幸福程度一样。神经网络的学习通过某个指标表示现在的状态，然后以这个状态为基准，寻找最优权重参数。神经网络中所使用的指标就是损失函数。**损失函数是衡量网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。**如果给损失函数乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”。\n",
    "## 4.2.1 均方误差\n",
    "均方误差作为损失函数如下：\n",
    "$$E = \\frac{1}{2}\\sum_k(y_k-t_k)^2$$\n",
    "其中$y_k$表示神经网络的输出，$t_k$表示监督数据，$k$表示数据的维数。\n",
    "## 4.2.2 交叉熵损失\n",
    "交叉熵作为损失函数如下：\n",
    "$$E = -\\sum_k t_klogy_k$$\n",
    "$t_k$实际中一般为one-hot标签，所以交叉熵损失实际上计算的是对应正确解标签的输出的自然对数的负值，这个值越接近0越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # 避免出现log 0\n",
    "    return -np.sum(t*np.log(y+delta))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 mini-batch学习\n",
    "机器学习使用训练数据进行学习，也就是使用训练数据计算损失函数的值，找出使该值尽可能小的参数，因此计算损失函数必须将所有的训练数据作为对象。也就是如果由N个数据的话，我们就要把这N个数据的损失函数的总和或算数平均作为学习的目标。\n",
    "\n",
    "在实验中，如果遇到海量的数据，比如几百万，几千万之多，这种情况下以全体数据作为对象计算损失函数是不现实的。因此，在实际中，我们会从全部数据中随机选出一部分，作为全部数据的“近似”，也就是从训练数据中选出一批数据(称为mini-batch，小批量)，然后对每个mini-batch进行学习，这种学些方式称为mini-batch学习，mini-batch学习的过程中也用到了批处理的思想。（这种方法就是统计学中的，以抽样样本的情况来代表总体样本的情况）\n",
    "## 4.2.4 为什么需要损失函数（重要）\n",
    "我们来关注神经网络中的某一个权重参数，此时，对该权重参数的损失函数求导，表示的就是“**如果稍微改变这个权重参数的值，损失函数的值如何变化**”。如果导数的值为负数，通过该权重参数向正方向改变，可以减小损失函数的值；反过来，导数值为正，权重参数向负方向改变，也可以减小损失函数的值。当导数为0时，无论权重参数向哪个方向改变，损失函数的值都不会改变，此时该权重参数的更新会停留在此处。\n",
    "\n",
    "**注意：在神经网络的学习中，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0**\n",
    "\n",
    "## 4.3 数值微分\n",
    "### 4.3.1 导数\n",
    "采用微小的差分实现求导数二点过程称为数值微分\n",
    "### 4.3.2 偏导数\n",
    "自行复习\n",
    "## 4.4 梯度\n",
    "**由全部变量组成的向量称为梯度**\n",
    "梯度有以下特征：\n",
    "1. 梯度指向各点处函数值降低的地方，更严格地说，梯度指示的方向是各点处函数值减小最多的方向\n",
    "2. 越靠近鞍点，梯度的大小越大\n",
    "### 4.4.1 梯度法\n",
    "梯度法是寻找函数最大值或最小值的方法：\n",
    "\n",
    "在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断地沿着梯度方向前进。\n",
    "\n",
    "数学表示:\n",
    "$$ x_0 = x_0-\\eta \\frac{\\varphi f}{\\varphi x_0} $$\n",
    "\n",
    "其中$\\eta$称为学习率，需要人为确定，学习率过大或过小，都无法使梯度法达到一个“好的位置”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2 0.3 0.5]\n",
      " [0.8 0.1 0.1]\n",
      " [0.6 0.1 0.3]]\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "[0.8 0.1 0.1]\n",
      "[1 0 0]\n",
      "[[0.2 0.3 0.5]\n",
      " [0.8 0.1 0.1]\n",
      " [0.6 0.1 0.3]]\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "0.645980451468738\n",
      "[[0.8 0.1 0.1]]\n",
      "[[1 0 0]]\n",
      "0.22314342631421757\n"
     ]
    }
   ],
   "source": [
    "# mini-batch版的cross entropy error\n",
    "import numpy as np\n",
    "def cross_entropy_error(y, t): # one-hot版本\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    print(y)\n",
    "    print(t)    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y+1e-7))/batch_size\n",
    "\n",
    "\n",
    "y1 = np.array([[0.2,0.3,0.5],[0.8,0.1,0.1],[0.6,0.1,0.3]])\n",
    "t1 = np.array([[0,1,0],[1,0,0],[1,0,0]])\n",
    "print(y1)\n",
    "print(t1)\n",
    "y2 = np.array([0.8,0.1,0.1])\n",
    "t2 = np.array([1,0,0])\n",
    "print(y2)\n",
    "print(t2)\n",
    "\n",
    "print(cross_entropy_error(y1,t1))\n",
    "print(cross_entropy_error(y2,t2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "# 一元函数的数值微分实例\n",
    "import numpy as np\n",
    "\n",
    "def function_1(x):\n",
    "    return 0.01*x**2+0.1*x\n",
    "\n",
    "def numerical_differentiation(f,x):\n",
    "    h = 1e-4 # 使用程序实现数值微分，差分采用0.0001即可获得很好的效果\n",
    "    return (f(x+h)-f(x-h))/(2*h)\n",
    "\n",
    "\n",
    "print(numerical_differentiation(function_1,5))\n",
    "print(numerical_differentiation(function_1,10))\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size): # 轮流计算每个偏导数\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        x[idx] = tmp_val+h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val-h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1-fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad\n",
    "\n",
    "def gradient_descent(f, init_x, lr = 0.01, step_num =100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = gradient_descent(f, x)\n",
    "        x -= lr*grad\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 学习算法的实现\n",
    "\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两层神经网络的实现\n",
    "import sys, os\n",
    "sys.path.append(os.curdir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size,output_size,weight_init_std=0.01): # 初始化模型\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b1'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        a1 = np.dot(x, self.params['W1'])+self.params['b1']\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, self.params['W2'])+self.params['b2']\n",
    "        z2 = softmax(a2)\n",
    "\n",
    "        return z2\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
