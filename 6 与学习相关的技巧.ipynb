{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 参数的更新\n",
    "本节介绍最优化的方法\n",
    "## 6.1.3 SGD的优缺点\n",
    "SGD的优点是简单，易于实现。但是对于一些形状是非均向，比如呈延伸状的函数，那么对于这些函数，SGD搜索的效率就会非常低，在函数等高线上体现很可能就是“之”字型的搜索。而SGD非常低效的本质原因是：$$梯度的方向并不一定指向最小值的方向$$\n",
    "\n",
    "下面将会介绍Momentum、AdaGrad、Adam这三种方法来取代SGD\n",
    "## 6.1.4 Momentum（动量）\n",
    "Momentum方法和物理中的“动量”有关，用数学式表示Momentum方法就是:\n",
    "$$v = av - \\eta\\frac{\\varphi L}{\\varphi W}$$\n",
    "$$W = W + v$$\n",
    "相对于SGD出现的新变量$v$，对应物理上的速度，上述表达式\n",
    "\n",
    "## 6.1.5 AdaGrad\n",
    "在学习率的技巧中，有一种关于**学习率衰减**的方法，随着学习的进行，使学习率逐渐减小。逐渐减小学习率的方法，相当于将“全体参数”的学习率值一起减小，而AdaGrad进一步发展了这个思想，针对不同的参数，采用不同的学习率去更新。\n",
    "$$h = h +\\frac{\\varphi L}{\\varphi W}\\cdot \\frac{\\varphi L}{\\varphi W}(dimension wise)$$\n",
    "$$W = W - \\eta\\frac{1}{\\sqrt{h}}\\frac{\\varphi L}{\\varphi W}$$\n",
    "\n",
    "AdaGrad会记录以前所有梯度的平方的和，记录为h。更新参数时，通过乘以$\\frac{1}{\\sqrt{h}}$，就可以针对不同的的参数调整学习的尺度。并且参数中更新幅度较大的元素的学习率将变小。\n",
    "\n",
    "注意：因为AdaGrad会记录过去所有梯度的平方的和，如果无止境地学习，那么更新量就会变为0，为了避免这种情况，可以使用RMSProp方法。RMSProp不会将过去所有的梯度相加，而是逐渐以往过去的梯度，做加法运算时将新梯度的信息更多地反映出来\n",
    "\n",
    "\n",
    "## 6.1.6 Adam\n",
    "Adam结合了AdaGrad和Momentum的优点。\n",
    "\n",
    "事实中，没有哪一种优化方法能够很好地适用于所有的情况，每种优化方法都有各自适应的情况，我们应该根据实际情况选择。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 权重的初始值\n",
    "\n",
    "\n",
    "对于Sigmoid等S型函数，使用Xavier权重来初始化权重参数。激活函数为ReLU等函数时，使用He权重来初始化权重参数。\n",
    "如果前一层的节点个数为n，那么对于Xavier来说，权重分布为标准差为$\\frac{1}{\\sqrt{n}}$的高斯分布，对于He权重来说，权重分布为标准差为$\\frac{2}{\\sqrt{n}}$的高斯分布。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Batch Normalization\n",
    "Batch Normalization顾名思义,就是按照mini-batch对数据进行正规化操作，Batch Norm的目的是调整各层激活值的分布，使其具有一个较高的广度，具体来说就是将数据正规化为均值为0，方差为1的分布。也因此，Batch Norm在神经网络中会作为一个层来实现。Batch Norm虽然是在2015年提出的新算法，但是它已经被用在各种各样的研究中了。\n",
    "\n",
    "它具有如下的优点：\n",
    "1. 加快学习过程\n",
    "2. 降低对权重参数初始值的敏感度\n",
    "3. 减少过拟合\n",
    "\n",
    "补充：关于数据分布的广度\n",
    "如果数据分布较为集中，那么说明多个神经元或神经层的工作可以由一个神经元或神经层完成，模型的“表现力”就会不足。如果数据分布又绝大多数分布在0或1处，那么可能会导致梯度消失问题。\n",
    "因此数据分布应该具有一个良好的广度。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 过拟合\n",
    "在机器学习的过程中，过拟合是一个常见的也是非常需要解决的问题。过拟合是指模型能够非常好地拟合训练数据，而不能很好地拟合不在训练集中的数据地状态，称为过拟合。\n",
    "\n",
    "## 6.4.1 过拟合\n",
    "过拟合地主要原因有两个：\n",
    "1. 模型参数太多，拟合能力过强\n",
    "2. 数据过少\n",
    "\n",
    "\n",
    "## 6.3.2 权重衰减\n",
    "权值衰减是一直以来用来减轻过拟合的一种方法。该方法通过对学习过程中的较大权值进行惩罚，来抑制过拟合。例如，可以为损失函数加上权重的平法范数（L2范数，对于权重W=(w_1,w_2,w_3...,w_n)，则L2范数可用$ \\sqrt{w_1^2,w_2^2,...,w_n^n}$ ），如果权重为W，那么L2范数的权值衰减就是$\\frac{1}{2}\\lambda W^2$，通过误差反向传播后，要为反向传播的结果加上正则化项的导数$\\lambda W$\n",
    "\n",
    "## 6.3.3 Dropout\n",
    "权值衰减可以应付一些比较简单的模型，但是对于复杂的模型，权值衰减的效果就没那么好了，这时就要采用Dropout方法了。\n",
    "\n",
    "Dropout是一种在训练过程中随机删除神经元的方法，在每一次的学习过程中，随机选出隐藏层的神经元，然后将其删除。然后在推理时，虽然会传递所有神经元的信号，但是对于各个神经元的输出，要乘上训练时的删除比例再输出。\n",
    "\n",
    "注意：Dropout可以看作是机器学习中的集成学习方法（集成学习就是让多个模型单独进行学习，然后在推理时，输出多个模型的平均值），Dropout的过程中每一次学习时，都会删除一些神经元，这个过程可以看作一个新模型的构建。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 超参数的验证\n",
    "神经网络中，除了权重和偏置等参数，还有超参数（比如各层神经元的数量、batch的大小、参数更新时的学习率或权值衰减等）。如果这些超参数没有设置合适的值，模型的性能就会很差。而且要注意，在超参数的选择过程中一般会伴有很多的试错。\n",
    "## 6.5.1 验证数据\n",
    "在调整超参数时要确定评估超参数性能的数据，这个数据我们叫做验证数据。验证数据是用于超参数的性能评估。\n",
    "\n",
    "注意：验证数据不能采用测试数据，验证数据一般单独划分，或者采用训练数据中的一部分。\n",
    "\n",
    "## 6.5.2 超参数的优化方法\n",
    "0. 确定超参数的优化范围\n",
    "1. 从设定的超参数范围中随机采样\n",
    "2. 使用1中得到的超参数的值进行学习，通过验证数据评估精度\n",
    "3. 重复步骤1，2根据识别精度，缩小超参数的范围"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
